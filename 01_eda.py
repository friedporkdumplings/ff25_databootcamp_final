# -*- coding: utf-8 -*-
"""01_eda.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mUlVSKySao8OYsiMoXqMifSxWBVLdVb

This section performs data collection and basic cleaning required for exploratory analysis.
No feature engineering or modeling decisions are made here.
"""

# loading libraries
import numpy as np
import pandas as pd
import requests
import time

# visualisation
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import plot_importance


# processing data
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# regression models
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor

# score
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score

# date transforming
from datetime import datetime
import datetime as dt

# xgboost
from xgboost import XGBClassifier, XGBRegressor

# tqdm
from tqdm import tqdm

# api imformation
api_key = 'ac7c5b21-cd9e-4ed4-ab9d-c9f78c854a03'
BASE_URL = "https://api.balldontlie.io/v1"
headers = {"Authorization": api_key}

# this will write a function that we get all endpoints from this function so the error will be easy to see
def api_get(endpoint, params=None):
    url = f"{BASE_URL}/{endpoint}"
    r = requests.get(url, params=params, headers=headers)

    try:
        r.raise_for_status()
    except Exception as e:
        print("HTTP error:", r.status_code)
        print("Response:", r.text[:300])
        raise e

    try:
        return r.json()
    except:
        print("JSON decode error. Raw response:")
        print(r.text[:300])
        raise

def get_stats_range(start_date, end_date, per_page=100, max_retry=5):

    print(f"Fetching stats {start_date} → {end_date}")

    all_rows = []
    cursor = None
    batch_id = 0

    while True:
        batch_id += 1

        params = {
            "start_date": start_date,
            "end_date": end_date,
            "per_page": per_page,
        }
        if cursor:
            params["cursor"] = cursor

        # retry block
        data = None
        for attempt in range(max_retry):
            try:
                data = api_get("stats", params)
                break
            except Exception:
                print(f"Retry {attempt+1}/{max_retry} at cursor={cursor}")
                time.sleep(1)

        if data is None:
            print("Failed too many times. Stopping.")
            break

        batch = data.get("data", [])
        if not batch:
            print("Empty batch. Done.")
            break

        all_rows.extend(batch)
        print(f"Batch {batch_id:03d}: +{len(batch)} rows (total={len(all_rows)})")

        cursor = data.get("meta", {}).get("next_cursor")
        if not cursor:
            print("No next cursor. Finished.")
            break

    df = pd.json_normalize(all_rows)

    if "game.date" in df.columns:
        df["game.date"] = pd.to_datetime(df["game.date"])

    return df

def clean_stats(df):
    df = df.copy()

    # drop duplicate player-game rows
    df = df.drop_duplicates(subset=["player.id", "game.id"])

    # convert game.date
    df["game.date"] = pd.to_datetime(df["game.date"])

    # convert minutes ("35" or "35:22") into float
    def parse_min(x):
        if isinstance(x, str):
            if ":" in x:
                m, s = x.split(":")
                return float(m) + float(s)/60
            return float(x)
        return float(x)

    df["min"] = df["min"].apply(parse_min)

    # missing draft → fill with 0
    df["player.draft_year"] = df["player.draft_year"].fillna(0).astype(int)
    df["player.draft_round"] = df["player.draft_round"].fillna(0).astype(int)
    df["player.draft_number"] = df["player.draft_number"].fillna(0).astype(int)

    # drop useless OT3 columns if exist
    for col in ["game.home_ot3", "game.visitor_ot3", "game.datetime"]:
        if col in df.columns:
            df = df.drop(columns=[col], errors="ignore")

    return df

START_DATE = "2025-10-01"
END_DATE = "2025-12-01"

df_raw = get_stats_range(
    start_date=START_DATE,
    end_date=END_DATE
)

print(df_raw.shape)
df_raw.head()

"""Analysing Data"""

# cleaning before checking
df = clean_stats(df_raw)

print(df.shape)
df.head()

# checking if there is null data and Dtype
df.info()
df.describe().T

# most variables contain no missing values, indicating high overall data completeness.
missing = df.isna().mean().sort_values(ascending=False)
missing[missing > 0]

# overtime-related variables show very high missing rates, reflecting the rarity of overtime games
missing.loc[
    ["game.home_ot1", "game.home_ot2", "game.visitor_ot1", "game.visitor_ot2"]
]

# The median points scored is low and have a lot 0, while numerous high-value observations create a long right tail
plt.figure(figsize=(6,4))
sns.histplot(df["pts"], bins=50)
plt.title("Distribution of points scored per game")
plt.show()

# The distribution of points scored per game exhibits a pronounced long tail, with the 90th, 95th, and 99th percentiles at 20, 25, and 35 points respectively
df["pts"].quantile([0.9, 0.95, 0.99])

"""Summary

This exploratory data analysis is based on 10,779 player–game observations from the 2025 NBA season, with a well-defined structure and diverse variable types.
Most variables exhibit no missing values, indicating high overall data completeness; overtime-related variables show high missing rates, reflecting the rarity of overtime games rather than data quality issues.
Points scored (pts) and minutes played (min) display strongly right-skewed distributions, with most observations concentrated at low values and a small number of high-usage performances forming a long tail.
Quantile analysis further confirms that high-scoring games constitute a small fraction of observations and represent genuine but rare events.
Overall, the dataset captures the inherent imbalance in player participation and performance in NBA games and provides a solid foundation for subsequent feature engineering and modeling.

## Save Raw Data

The cleaned dataset is saved as a raw data snapshot for reproducibility.
This file does not include any feature engineering or modeling-related transformations.
"""

import os
os.getcwd()

import os

os.makedirs("data", exist_ok=True)
df.to_csv("data/df_raw_v1.csv", index=False)

print("Saved to /content/data/df_raw_v1.csv")

df.to_csv("data/raw_data.csv", index=False)